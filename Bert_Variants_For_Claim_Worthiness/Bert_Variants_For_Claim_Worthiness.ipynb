{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    #sys.path.insert(0, \"..\")\n",
    "    sys.path.append('../')\n",
    "import logging\n",
    "logging.basicConfig(level='ERROR')\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import wandb\n",
    "import wget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import (AutoModel, AutoTokenizer, AutoModelForSequenceClassification, \n",
    "    get_linear_schedule_with_warmup, AutoConfig)\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, log_loss,\n",
    "matthews_corrcoef, average_precision_score)\n",
    "\n",
    "#Custom modules\n",
    "import utils\n",
    "from utils import custom_models, early_stopping, worthiness_checker, constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = 'TransformersForClaimWorthiness.ipynb'\n",
    "\n",
    "# Constants\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "parent_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "seed_list = [7, 42] # seed_list = [7, 42, 127]\n",
    "fold_count = 3 #5\n",
    "patience=5\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "metric_types = np.dtype(\n",
    "    [\n",
    "        (\"mAP\", float),\n",
    "        (\"auc\", float),\n",
    "        (\"accuracy\", float),\n",
    "        (\"precision\", float),\n",
    "        (\"recall\", float),\n",
    "        (\"f1\", float),\n",
    "        (\"mcc\", float),\n",
    "        (\"log_loss\", float),\n",
    "        (\"loss\", float)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = constants.Constants()\n",
    "constants.device = device\n",
    "constants.parent_dir = parent_dir\n",
    "constants.seed_list = seed_list\n",
    "constants.fold_count = fold_count\n",
    "constants.patience = patience\n",
    "constants.loss_function = loss_function\n",
    "constants.metric_types = metric_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(data_version):\n",
    "    train_df = pd.read_csv(os.path.join(parent_dir, 'Data','train_english_{}.tsv'.format(data_version)), delimiter='\\t')\n",
    "    test_df = pd.read_csv(os.path.join(parent_dir, 'Data','test_english_{}.tsv'.format(data_version)), delimiter='\\t')\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_list(df: pd.DataFrame, fold_count, random_state):\n",
    "    # Group positive and negative samples for stratified sampling\n",
    "    sampling_df = df.sample(frac=1, replace=False, random_state=random_state)\n",
    "    sampling_negative_df = sampling_df[sampling_df['check_worthiness']==0]\n",
    "    sampling_positive_df = sampling_df[sampling_df['check_worthiness']==1]\n",
    "\n",
    "    #determine fold length for both classes\n",
    "    fold_size_for_negatives = sampling_negative_df.shape[0]//fold_count\n",
    "    fold_size_for_positives = sampling_positive_df.shape[0]//fold_count\n",
    "\n",
    "    fold_list = []\n",
    "    for i in range(fold_count):\n",
    "        fold_for_negatives = sampling_positive_df.iloc[fold_size_for_positives*i : fold_size_for_positives * (i+1), :]\n",
    "        fold_for_positives = sampling_negative_df.iloc[fold_size_for_negatives*i : fold_size_for_negatives * (i+1), :]\n",
    "        fold_df = pd.concat([fold_for_negatives, fold_for_positives]).sample(frac=1, replace=False, random_state=random_state)\n",
    "\n",
    "        fold_list.append(fold_df)\n",
    "\n",
    "    return fold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_dataset(df, tokenizer, config):\n",
    "    max_token_length = config.max_token_length\n",
    "\n",
    "    sentences = df.tweet_text.values\n",
    "    labels = df.check_worthiness.values\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]' or equivalent\n",
    "                            max_length = max_token_length,           # 64? 4-128? Pad & truncate all sentences.\n",
    "                            truncation=True,\n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = False,   # Do not Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    labels = torch.tensor(labels).float()\n",
    "    dataset = TensorDataset(input_ids, labels)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'name': 'Bert_hyperparameters',\n",
    "    'method': 'bayes', #grid, random\n",
    "    'program': 'TransformersForClaimWorthiness.ipynb',\n",
    "    'early_terminate': {\n",
    "      'type': 'hyperband',\n",
    "      'eta': 3,\n",
    "      's': 2,\n",
    "      'max_iter': 27   \n",
    "    },\n",
    "    'metric': {\n",
    "      'name': 'avg_val_mAP',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'data_version': {\n",
    "          'values': ['raw', 'cleaned_with_mentions', 'cleaned_without_mentions'],\n",
    "          'distribution': 'categorical'  \n",
    "        },\n",
    "        'max_token_length': {\n",
    "           'min': 4,\n",
    "           'max': 80,\n",
    "           'distribution': 'int_uniform'\n",
    "        },\n",
    "        'model_name': {\n",
    "          'values': ['bert-base-uncased'],\n",
    "          'distribution': 'categorical'  \n",
    "        },\n",
    "        'hidden_act': {\n",
    "          'values': ['relu', 'gelu', 'gelu_new', 'silu'],\n",
    "          'distribution': 'categorical'  \n",
    "        },\n",
    "        'position_embedding_type': {\n",
    "          'values': ['absolute', 'relative_key', 'relative_key_query'],\n",
    "          'distribution': 'categorical'  \n",
    "        },\n",
    "        'attention_dropout': {\n",
    "            'min': 0.001,\n",
    "            'max': 0.2\n",
    "        },\n",
    "        'transformer_dropout': {\n",
    "            'min': 0.001,\n",
    "            'max': 0.2\n",
    "        },\n",
    "        'classifier_dropout': {\n",
    "            'min': 0.001,\n",
    "            'max': 0.2\n",
    "        },\n",
    "        'layer_norm_eps': {\n",
    "            'min': 1e-14,\n",
    "            'max': 1e-10\n",
    "        },\n",
    "        'batch_size': {\n",
    "           'min': 2,\n",
    "           'max': 80,\n",
    "           'distribution': 'int_uniform'\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'min': 0.0000005,\n",
    "            'max': 0.00025\n",
    "        },\n",
    "        'epochs':{\n",
    "           'min': 1,\n",
    "           'max': 80,\n",
    "           'distribution': 'int_uniform'\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check_points = ['vinai/bertweet-covid19-base-uncased', 'roberta-base', 'bert-base-uncased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_defaults = {\n",
    "    'data_version': 'cleaned_with_mentions',\n",
    "    'max_token_length': 46,\n",
    "    'model_name': 'bert-base-uncased',\n",
    "    'hidden_act': 'gelu',\n",
    "    'position_embedding_type': 'absolute',\n",
    "    'layer_norm_eps': 5.4225686692811365e-11,\n",
    "    'learning_rate': 0.000028734737822604655,\n",
    "    'transformer_dropout': 0.03873251195245608,\n",
    "    'attention_dropout': 0.015328152075297112,\n",
    "    'classifier_dropout': 0.10850207289443518,\n",
    "    'batch_size': 53,\n",
    "    'epochs':25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANDB PARAMETER\n",
    "def ret_dataloader(batch_size, train_dataset, validation_dataset):\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,  # The training samples.\n",
    "                sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "                batch_size = batch_size # Trains with this batch size.\n",
    "            )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "                validation_dataset, # The validation samples.\n",
    "                sampler = SequentialSampler(validation_dataset), # Pull out batches sequentially.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            )\n",
    "    return train_dataloader, validation_dataloader\n",
    "\n",
    "def ret_optim(model, config):\n",
    "    #print('Learning_rate = ',wandb.config.learning_rate )\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                      lr = config.learning_rate, \n",
    "                      eps = 1e-8 \n",
    "                    )\n",
    "    return optimizer\n",
    "    \n",
    "def ret_scheduler(train_dataloader,optimizer, config):\n",
    "    epochs = config.epochs\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "    return scheduler\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(fold_list, fold_index, tokenizer, config):\n",
    "    temp_list = fold_list.copy()\n",
    "\n",
    "    trial_validation_df = temp_list.pop(fold_index)\n",
    "    trial_train_df = pd.concat(temp_list)\n",
    "\n",
    "    train_dataset = create_dataset(trial_train_df, tokenizer, config)\n",
    "    validation_dataset = create_dataset(trial_validation_df, tokenizer, config)\n",
    "\n",
    "    return ret_dataloader(config.batch_size, train_dataset, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_metrics(probability, label_list):\n",
    "    metrics_dictionary = {}\n",
    "    # predictions = np.argmax(probability.detach().cpu().numpy(), axis=0)\n",
    "    predictions =  [int(i > .5) for i in probability]\n",
    "\n",
    "    accuracy = accuracy_score(label_list, predictions)\n",
    "    precision = precision_score(label_list, predictions, zero_division=0)\n",
    "    recall = recall_score(label_list, predictions, zero_division=0)\n",
    "    f1 = f1_score(label_list, predictions, zero_division=0)\n",
    "    log_loss = metrics.log_loss(label_list, predictions)\n",
    "    mcc = matthews_corrcoef(label_list, predictions)\n",
    "    auc = roc_auc_score(label_list, probability)\n",
    "    \n",
    "    mAP = average_precision_score(label_list, probability)\n",
    "\n",
    "    metric_df = pd.DataFrame(np.empty(0, dtype=metric_types))\n",
    "    metric_df.loc[0] = [mAP, auc, accuracy, precision, recall, f1, mcc, log_loss, 0]\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(metric_df, prefix):\n",
    "    wandb.log({prefix + 'mAP':metric_df.loc[0, 'mAP'],\n",
    "            prefix + 'auc':metric_df.loc[0, 'auc'],\n",
    "            prefix + 'accuracy':metric_df.loc[0, 'accuracy'],\n",
    "            prefix + 'precision':metric_df.loc[0, 'precision'],\n",
    "            prefix + 'recall':metric_df.loc[0, 'recall'],\n",
    "            prefix + 'f1':metric_df.loc[0, 'f1'],\n",
    "            prefix + 'mcc':metric_df.loc[0, 'mcc'],\n",
    "            prefix + 'log_loss':metric_df.loc[0, 'log_loss'],\n",
    "            prefix + 'eval_loss':metric_df.loc[0, 'loss']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(model, device, dataloader):\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        probability_list = torch.Tensor(0)\n",
    "        label_list = np.empty(0)\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in dataloader:\n",
    "            \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_labels = batch[1].to(device)\n",
    "\n",
    "            with torch.no_grad():        \n",
    "                probability = model(b_input_ids).flatten()\n",
    "                loss = loss_function(probability, b_labels)\n",
    "\n",
    "            # Accumulate the validation loss, probability and labels.\n",
    "            total_eval_loss += loss.item()\n",
    "            probability_list = torch.cat((probability_list, probability.detach().cpu()), axis=0)\n",
    "            label_list = np.concatenate((label_list, batch[1]), axis=0)\n",
    "\n",
    "            # Move probability and labels to CPU\n",
    "            probability = probability.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # del b_input_ids\n",
    "            # del b_labels\n",
    "\n",
    "        # Calculate and log metrics and loss.\n",
    "        metrics_df = get_metrics(probability_list, label_list)\n",
    "        metrics_df.loc[0,'loss'] = total_eval_loss / len(dataloader)\n",
    "\n",
    "        return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, device, dataloader, loss_function, optimizer, scheduler):\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        probability = model(b_input_ids)\n",
    "        loss = loss_function(probability.flatten(), b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # del b_input_ids\n",
    "        # del b_labels\n",
    "\n",
    "    return total_train_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validate():\n",
    "    # clean gpu memory in any case if previous wandb run was crashed.\n",
    "    torch.cuda.empty_cache()\n",
    "    run = wandb.init(config=sweep_defaults)\n",
    "    run_start_time = time.time()\n",
    "    # print(wandb.config.items())\n",
    "    epochs = wandb.config.epochs\n",
    "\n",
    "    model_name = wandb.config.model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "    train_df, test_df = get_data_from_file(wandb.config.data_version)\n",
    "\n",
    "    run_train_metrics_list = []\n",
    "    run_val_metrics_list = []\n",
    "\n",
    "    for seed_index, seed in enumerate(seed_list):\n",
    "\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        fold_list = get_fold_list(train_df, 3, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "        for fold_index in range(len(fold_list)): \n",
    "\n",
    "            train_dataloader, validation_dataloader = create_dataloaders(fold_list, fold_index, tokenizer, wandb.config)\n",
    "            model = custom_models.TransformerClassifier(wandb.config).to(device)\n",
    "\n",
    "            optimizer = ret_optim(model, wandb.config)\n",
    "            scheduler = ret_scheduler(train_dataloader, optimizer, wandb.config)\n",
    "\n",
    "            # Creating class that checks early stopping condition\n",
    "            early_stopping = modules.early_stopping.EarlyStopping(patience=patience)\n",
    "\n",
    "            epoch_train_metrics_list = []\n",
    "            epoch_val_metrics_list = []\n",
    "\n",
    "            for epoch_i in range(0, epochs):\n",
    "\n",
    "                # ========================================\n",
    "                #               Training\n",
    "                # ========================================\n",
    "\n",
    "                # print(\"\")\n",
    "                print('======== Seed {:} - Fold {:} - Epoch {:} / {:} ========'.format(seed_index+1, fold_index+1, epoch_i + 1, epochs))\n",
    "                # print('Training...')\n",
    "                training_start_time = time.time()\n",
    "\n",
    "                epoch_train_loss = train_one_epoch(model, device, train_dataloader, loss_function, optimizer, scheduler)\n",
    "\n",
    "                training_time = format_time(time.time() - training_start_time)\n",
    "                wandb.log({'train_loss_':epoch_train_loss})\n",
    "\n",
    "                # print(\"  Average training loss: {0:.2f}\".format(epoch_train_loss))\n",
    "                #print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "                # ========================================\n",
    "                #               Evaluation\n",
    "                # ========================================\n",
    "\n",
    "                # print(\"Running Evaluation...\")\n",
    "                evaluation_start_time = time.time()\n",
    "\n",
    "                epoch_train_metrics = evaluate_one_epoch(model, device, train_dataloader)\n",
    "                log_metrics(epoch_train_metrics, 'train_')\n",
    "                epoch_train_metrics_list.append(epoch_train_metrics)\n",
    "\n",
    "                epoch_val_metrics = evaluate_one_epoch(model, device, validation_dataloader)\n",
    "                log_metrics(epoch_val_metrics, 'val_')\n",
    "                epoch_val_metrics_list.append(epoch_val_metrics)\n",
    "\n",
    "                val_mAP = epoch_val_metrics['mAP'].loc[0]\n",
    "                train_mAP = epoch_train_metrics['mAP'].loc[0]\n",
    "\n",
    "                print(\"  Training mAP: {:.3f} - Validation mAP: {:.3f}\".format(train_mAP,val_mAP ))\n",
    "\n",
    "                evaluation_time = format_time(time.time() - evaluation_start_time)\n",
    "                #print(\"  Evaluation took: {:}\".format(evaluation_time))\n",
    "                if early_stopping.should_stop(val_mAP):\n",
    "                    # print('terminating because of early stopping!')\n",
    "                    wandb.log({'early_stopped_at': seed_index*len(fold_list)*epochs+fold_index*epochs+epoch_i + 1})\n",
    "                    break\n",
    "\n",
    "            # at the end of each fold, after every epoch finished, \n",
    "            # get last epoch`s metrics as final metrics of current fold\n",
    "            #  # for  df in epoch_val_metrics_list:\n",
    "            #  #     log_metrics(epoch_val_metrics, 'val_{:}_{:}_'.format(seed_index+1, fold_index+1))\n",
    "\n",
    "            fold_train_metrics = epoch_train_metrics_list[-1]\n",
    "            fold_val_metrics = epoch_val_metrics_list[-1]\n",
    "\n",
    "            run_train_metrics_list.append(fold_train_metrics)\n",
    "            run_val_metrics_list.append(fold_val_metrics)\n",
    "\n",
    "            # at the end of every fold, \n",
    "            # calculate average metrics as final metrics of current run\n",
    "            run_train_metrics = pd.concat(run_train_metrics_list)\n",
    "            run_val_metrics = pd.concat(run_val_metrics_list)\n",
    "\n",
    "            log_metrics(pd.DataFrame([run_train_metrics.mean()]), 'avg_train_')\n",
    "            log_metrics(pd.DataFrame([run_val_metrics.mean()]), 'avg_val_')\n",
    "\n",
    "            # del model\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "    # print(\"\")\n",
    "    # print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-run_start_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project=\"Transformers_For_ClaimWorthiness\"\n",
    "entity=\"cemulu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_id = wandb.sweep(sweep_config, project=project)\n",
    "sweep_id = 'nbovtee3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, project=project,function=cross_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model with best config and whole training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "# best_sweep = '2afv0m0i' #bertweet\n",
    "# best_sweep = 'embywnlj' #roberta\n",
    "best_sweep = 'nbovtee3' #bert\n",
    "sweep = api.sweep(\"cemulu/Transformers_For_ClaimWorthiness/\" + best_sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by -summary_metrics.avg_val_mAP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7651173954688729"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run = sweep.best_run()\n",
    "best_run.summary.get(\"avg_val_mAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch configuration of the best run:\n",
      "36\n",
      "Early stopped at:\n",
      "                   36    66    123    156    192    240\n",
      "fold_index         1.0   2.0   3.0    4.0    5.0    6.0\n",
      "cumulative_epoch  12.0  45.0  90.0  118.0  155.0  195.0\n",
      "epoch_of_fold     12.0   9.0  18.0   10.0   11.0   15.0\n",
      "\n",
      "Average epoch used as a reference for early stopping:  8\n"
     ]
    }
   ],
   "source": [
    "worthiness_checker = worthiness_checker.WorthinessChecker(best_run, constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 8 ========\n",
      "  Training mAP: 0.845 - Test mAP: 0.660\n",
      "======== Epoch 2 / 8 ========\n",
      "  Training mAP: 0.930 - Test mAP: 0.706\n",
      "======== Epoch 3 / 8 ========\n",
      "  Training mAP: 0.981 - Test mAP: 0.802\n",
      "======== Epoch 4 / 8 ========\n",
      "  Training mAP: 0.995 - Test mAP: 0.670\n",
      "======== Epoch 5 / 8 ========\n",
      "  Training mAP: 0.998 - Test mAP: 0.821\n",
      "======== Epoch 6 / 8 ========\n",
      "  Training mAP: 1.000 - Test mAP: 0.726\n",
      "======== Epoch 7 / 8 ========\n",
      "  Training mAP: 1.000 - Test mAP: 0.750\n",
      "======== Epoch 8 / 8 ========\n",
      "  Training mAP: 1.000 - Test mAP: 0.764\n",
      "*** Training Metrics ***\n",
      "        mAP       auc  accuracy  precision    recall        f1       mcc  \\\n",
      "0  0.845463  0.895281  0.841849   0.810078  0.720690  0.762774  0.647245   \n",
      "0  0.929843  0.955542  0.900243   0.866197  0.848276  0.857143  0.780621   \n",
      "0  0.980538  0.990913  0.951338   0.905844  0.962069  0.933110  0.895926   \n",
      "0  0.994603  0.998237  0.981752   0.956811  0.993103  0.974619  0.960788   \n",
      "0  0.997991  0.998982  0.986618   0.963455  1.000000  0.981387  0.971357   \n",
      "0  0.999505  0.999747  0.998783   0.996564  1.000000  0.998279  0.997342   \n",
      "0  0.999805  0.999896  0.998783   0.996564  1.000000  0.998279  0.997342   \n",
      "0  0.999952  0.999974  0.998783   0.996564  1.000000  0.998279  0.997342   \n",
      "\n",
      "   log_loss      loss  \n",
      "0  5.462385  0.417707  \n",
      "0  3.445511  0.285685  \n",
      "0  1.680747  0.171739  \n",
      "0  0.630282  0.070257  \n",
      "0  0.462208  0.058438  \n",
      "0  0.042019  0.017124  \n",
      "0  0.042019  0.013599  \n",
      "0  0.042019  0.012561  \n",
      "*** Test Metrics ***\n",
      "        mAP       auc  accuracy  precision    recall        f1       mcc  \\\n",
      "0  0.659986  0.742500  0.628571   0.571429  0.533333  0.551724  0.235702   \n",
      "0  0.705664  0.788333  0.671429   0.602941  0.683333  0.640625  0.342426   \n",
      "0  0.801824  0.830208  0.728571   0.648649  0.800000  0.716418  0.470898   \n",
      "0  0.670030  0.790417  0.692857   0.602410  0.833333  0.699301  0.423891   \n",
      "0  0.820828  0.846667  0.728571   0.641026  0.833333  0.724638  0.481531   \n",
      "0  0.725552  0.783750  0.707143   0.633803  0.750000  0.687023  0.420684   \n",
      "0  0.750253  0.765833  0.728571   0.689655  0.666667  0.677966  0.443705   \n",
      "0  0.764240  0.774375  0.757143   0.724138  0.700000  0.711864  0.502308   \n",
      "\n",
      "    log_loss      loss  \n",
      "0  12.828825  0.654265  \n",
      "0  11.348609  0.648394  \n",
      "0   9.374959  0.705280  \n",
      "0  10.608527  1.026218  \n",
      "0   9.374971  1.089322  \n",
      "0  10.115076  1.248984  \n",
      "0   9.374914  1.057967  \n",
      "0   8.388080  1.024209  \n"
     ]
    }
   ],
   "source": [
    "# optimized_model = worthiness_checker.train_full_model()\n",
    "\n",
    "model_file_name = 'vinai_bertweet-covid19-base-uncased_0.7651173954688729.pt'\n",
    "PATH = os.path.join(parent_dir, 'Model', model_file_name)\n",
    "worthiness_checker.load_model(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = worthiness_checker.config.model_name.replace('/','_')\n",
    "mAP = best_run.summary.get(\"avg_val_mAP\")\n",
    "\n",
    "PATH = os.path.join(parent_dir, 'Model','{}_{}.pt'.format(model_name, mAP))\n",
    "# torch.save(optimized_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"UK Health Minister Nadine Dorries has tested positive for COVID-19.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"i am positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"sheep is black\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"Recent research suggests that 15 percent of abortions are the result of coercion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = '''A Democratic bill negotiated between Sens. Joe Manchin and Chuck Schumer would \"increase taxes on millions of Americans across every income bracket.\"'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = '''Nancy Pelosi and Democrats \"want to turn 150 million Americans into felons overnight\" with HR 1808.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = '''John Fetterman wants to “eliminate life sentences for murderers.\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = '''\"The Sun is out of place, the Moon is out of place and the stars are out of place. The compasses are off\" because of a shift in the Earth’s poles.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"China threatens to shoot Nancy Pelosi’s plane down if she visits Taiwan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"In Virginia, we actually do protect same-sex marriage.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'worthiness_checker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\repos\\huggingface_test\\TransformersForClaimWorthiness\\TransformersForClaimWorthiness.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/repos/huggingface_test/TransformersForClaimWorthiness/TransformersForClaimWorthiness.ipynb#ch0000040?line=0'>1</a>\u001b[0m probability \u001b[39m=\u001b[39m worthiness_checker\u001b[39m.\u001b[39mpredict(tweet)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'worthiness_checker' is not defined"
     ]
    }
   ],
   "source": [
    "probability = worthiness_checker.predict(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007894571870565414"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_df = worthiness_checker.get_data_from_file(worthiness_checker.config.data_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>check_worthiness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POTUS wanted everyone to know he was in close ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who would you prefer to lead our nation’s resp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was a really really really really really re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bald-faced LIE. did self-quarantine until CDC ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIVE: Daily media briefing on COVID-19 with CO...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>This is how the COVID-19 is spreading and this...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Markets are crashing. Tourism is dying. Travel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Pray for Palestine. State of Emergency. - 27 c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>ADVISORY TO STUDENTS: Class suspensions were m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>The total Iranian COVID-19 case-count is in th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tweet_text  check_worthiness\n",
       "0    POTUS wanted everyone to know he was in close ...                 1\n",
       "1    Who would you prefer to lead our nation’s resp...                 0\n",
       "2    It was a really really really really really re...                 0\n",
       "3    Bald-faced LIE. did self-quarantine until CDC ...                 1\n",
       "4    LIVE: Daily media briefing on COVID-19 with CO...                 0\n",
       "..                                                 ...               ...\n",
       "135  This is how the COVID-19 is spreading and this...                 0\n",
       "136  Markets are crashing. Tourism is dying. Travel...                 0\n",
       "137  Pray for Palestine. State of Emergency. - 27 c...                 0\n",
       "138  ADVISORY TO STUDENTS: Class suspensions were m...                 0\n",
       "139  The total Iranian COVID-19 case-count is in th...                 1\n",
       "\n",
       "[140 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = worthiness_checker.create_dataset(test_df)\n",
    "_, test_dataloader = ret_dataloader(worthiness_checker.config.batch_size, _, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_batch(batch, self):\n",
    "    b_input_ids = batch[0].to(self.constants.device)\n",
    "    b_labels = batch[1].to(self.constants.device)\n",
    "    with torch.no_grad():        \n",
    "        probability = self.model(b_input_ids).flatten()\n",
    "        loss = self.constants.loss_function(probability, b_labels)\n",
    "    probability = probability.detach().cpu()\n",
    "    return probability, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_list = torch.Tensor(0)\n",
    "for batch in test_dataloader:\n",
    "    probability, _ = evaluate_one_batch( batch, worthiness_checker)\n",
    "    probability_list = torch.cat((probability_list, probability), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9889, 0.0067, 0.0076, 0.9908, 0.0075, 0.0071, 0.9879, 0.9914, 0.0065,\n",
       "        0.9908, 0.9913, 0.9867, 0.9901, 0.9905, 0.9790, 0.0092, 0.0063, 0.9905,\n",
       "        0.0066, 0.9863, 0.9875, 0.0076, 0.0067, 0.9907, 0.0539, 0.9901, 0.6011,\n",
       "        0.9912, 0.6066, 0.9866, 0.0067, 0.3569, 0.0078, 0.0072, 0.9907, 0.0078,\n",
       "        0.3646, 0.0071, 0.0067, 0.0064, 0.0109, 0.9908, 0.9882, 0.9826, 0.0080,\n",
       "        0.9905, 0.0066, 0.9871, 0.9907, 0.0792, 0.0142, 0.9906, 0.0067, 0.9916,\n",
       "        0.0087, 0.9911, 0.0064, 0.9911, 0.0067, 0.9878, 0.9899, 0.0075, 0.0063,\n",
       "        0.0063, 0.0083, 0.9866, 0.0076, 0.9911, 0.0071, 0.0134, 0.0067, 0.9909,\n",
       "        0.0264, 0.9913, 0.9914, 0.8241, 0.9866, 0.9915, 0.8571, 0.9859, 0.0071,\n",
       "        0.9863, 0.9897, 0.0068, 0.9432, 0.0069, 0.2583, 0.0066, 0.0070, 0.0081,\n",
       "        0.0136, 0.6670, 0.0118, 0.9520, 0.9906, 0.0065, 0.0066, 0.9844, 0.0076,\n",
       "        0.9890, 0.0094, 0.9792, 0.0095, 0.0084, 0.0068, 0.0068, 0.0297, 0.0069,\n",
       "        0.1405, 0.9886, 0.0071, 0.0065, 0.0074, 0.9911, 0.0065, 0.0065, 0.9864,\n",
       "        0.0067, 0.0065, 0.9906, 0.0065, 0.9888, 0.0199, 0.9876, 0.0509, 0.9845,\n",
       "        0.0072, 0.0073, 0.0065, 0.0064, 0.0064, 0.4466, 0.0071, 0.0081, 0.0073,\n",
       "        0.0091, 0.1433, 0.3149, 0.0063, 0.0086])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  [int(i > .5) for i in probability_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>check_worthiness</th>\n",
       "      <th>predictions</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POTUS wanted everyone to know he was in close contact with Gaetz and Collins today, both of whom were supposedly exposed to COVID-19. Did he look worried at the presser? No. It’s a message.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.988866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who would you prefer to lead our nation’s response to the growing COVID-19 threat?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was a really really really really really really really really really really really really really really really really really really really really bad idea to elect Donald Trump President of the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bald-faced LIE. did self-quarantine until CDC cleared him to return to work. WuFlu COVID-19 COVID-19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIVE: Daily media briefing on COVID-19 with COVID-19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>This is how the COVID-19 is spreading and this is how fast it spreads. We don't have a natural immunity to this virus or a vaccine to protect us. Something as simple as washing your hands can keep...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Markets are crashing. Tourism is dying. Travel is reduced to almost nothing, And World Financial Systems are in chaos. Because of the COVID-19 scare. And you cannot think Of a single reason Why so...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Pray for Palestine. State of Emergency. - 27 cases of COVID-19 - Major churches and mosques closed - No visitors allowed into Palestine - city of Bethlehem under lockdown - Border and Highways clo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.314902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>ADVISORY TO STUDENTS: Class suspensions were made for your safety. Please use those days to rest, strengthen your immune system, and read your lessons. Please don't gala muna. :)) WalangPasok COVI...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>The total Iranian COVID-19 case-count is in the hundreds of thousands, perhaps millions, according to my estimates (detailed at the link). This raises an important question: if there are two milli...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                  tweet_text  \\\n",
       "0              POTUS wanted everyone to know he was in close contact with Gaetz and Collins today, both of whom were supposedly exposed to COVID-19. Did he look worried at the presser? No. It’s a message.   \n",
       "1                                                                                                                         Who would you prefer to lead our nation’s response to the growing COVID-19 threat?   \n",
       "2    It was a really really really really really really really really really really really really really really really really really really really really bad idea to elect Donald Trump President of the...   \n",
       "3                                                                                                       Bald-faced LIE. did self-quarantine until CDC cleared him to return to work. WuFlu COVID-19 COVID-19   \n",
       "4                                                                                                                                                       LIVE: Daily media briefing on COVID-19 with COVID-19   \n",
       "..                                                                                                                                                                                                       ...   \n",
       "135  This is how the COVID-19 is spreading and this is how fast it spreads. We don't have a natural immunity to this virus or a vaccine to protect us. Something as simple as washing your hands can keep...   \n",
       "136  Markets are crashing. Tourism is dying. Travel is reduced to almost nothing, And World Financial Systems are in chaos. Because of the COVID-19 scare. And you cannot think Of a single reason Why so...   \n",
       "137  Pray for Palestine. State of Emergency. - 27 cases of COVID-19 - Major churches and mosques closed - No visitors allowed into Palestine - city of Bethlehem under lockdown - Border and Highways clo...   \n",
       "138  ADVISORY TO STUDENTS: Class suspensions were made for your safety. Please use those days to rest, strengthen your immune system, and read your lessons. Please don't gala muna. :)) WalangPasok COVI...   \n",
       "139  The total Iranian COVID-19 case-count is in the hundreds of thousands, perhaps millions, according to my estimates (detailed at the link). This raises an important question: if there are two milli...   \n",
       "\n",
       "     check_worthiness  predictions  probability  \n",
       "0                   1            1     0.988866  \n",
       "1                   0            0     0.006661  \n",
       "2                   0            0     0.007607  \n",
       "3                   1            1     0.990838  \n",
       "4                   0            0     0.007499  \n",
       "..                ...          ...          ...  \n",
       "135                 0            0     0.009059  \n",
       "136                 0            0     0.143284  \n",
       "137                 0            0     0.314902  \n",
       "138                 0            0     0.006276  \n",
       "139                 1            0     0.008649  \n",
       "\n",
       "[140 rows x 4 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['predictions'] = predictions\n",
    "test_df['probability'] = probability_list\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Positives:\n",
    "\n",
    "* (output = 0.986627) the number of COVID-19 cases in the US surpasses 1,000 with 1,004 people in 37 states and DC testing positive for COVID-19, plus 31 deaths. This is just beginning the acceleration phase of the Cor...\n",
    "\n",
    "* (output = 0.987125) Italy's Prime Minister Giuseppe Conte has announced that the whole of the country is being put on lockdown in an attempt to contain the COVID-19 outbreak. For the latest on COVID-19, click here:\n",
    "\n",
    "***\n",
    "\n",
    "* (output = 0.986261) The empire is striking back. The COVID-19 is now being used as a weapon to destabilize the US economy because that the powers that be feel that’s the only way they can get rid of Trump and regain ...\n",
    "\n",
    "* (output = 0.857124) As two epidemics - COVID-19 and Brexit - hit us, don’t let them make you forget: Priti Patel scandal and investigations of Johnson’s Arcuri Russia report referendum crimes lies incompetence etc et..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Negatives\n",
    "\n",
    "* (output = 0.006674) Democrats and the Media need to stop using the COVID-19 to politicize things and scare people. It's irresponsible. This is not the time to try and gain political points or headlines from scaring p...\n",
    "\n",
    "* (output = 0.006982) คำขวัญ Thailand 2020 No privacy No security No democracy No hope No future No mask But we had COVID-19 and stupid government Thank you ธนาธร ไม่เอารัฐประหาร\n",
    "\n",
    "***\n",
    "\n",
    "* (output = 0.006344) Italian doctor facing COVID-19 tsunami publishes long, moving thread that culminates with, ‘Is panic really worse than neglect and carelessness during an epidemic of this sort?’ Read the whole fri...\n",
    "\n",
    "* (output = 0.006693) This thread needs to fly. It shows how the legacy media is USING COVID-19 as a political weapon and even how the SAME reporters are contradicting themselves. This. Is. SICK.\n",
    "\n",
    "Ommitted Link: THREAD: Fri Jan 31, 2020, a few weeks before #Coronavirus has officially spread to other countries (which led to the bad stock market week Feb 24-Feb 28), the Trump Admin announced travel restrictions on China. Here is some of the reporting it generated. Take Politico of 2/4/20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet: \"\"private ny colleges: *closed college for the week because of the COVID-19* CUNY: *installed two new hand sanitizer dispensers*\"\"\n",
    "Label: 0\n",
    "Prediction: 1 with 98.75% probability\n",
    "\n",
    "Zero-shot GPT-3 response: \"The sentiment of this tweet is that private colleges are not doing enough to prevent the spread of COVID-19, while CUNY is taking steps to protect its students. This is not a check-worthy claim.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>check_worthiness</th>\n",
       "      <th>predictions</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Trust Merkel to be the first world leader to say openly what's what: the main point of social distancing measures (quarantines, closures, lockdowns) is to slow the spread of the COVID-19 so that h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The empire is striking back. The COVID-19 is now being used as a weapon to destabilize the US economy because that the powers that be feel that’s the only way they can get rid of Trump and regain ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.986261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>private ny colleges: *closed college for the week because of the COVID-19* CUNY: *installed two new hand sanitizer dispensers*</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.987550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>This thread needs to fly. It shows how the legacy media is USING COVID-19 as a political weapon and even how the SAME reporters are contradicting themselves. This. Is. SICK.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DEVELOPING: confirms directly to that he is self-quarantined after testing negative COVID-19. Meadows has not slowed down, with conference calls preparing for transition WH Chief of Staff. Meadows...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Older adults and people of all ages with severe chronic medical conditions are more likely to develop serious outcomes, including death, if infected with COVID-19. See CDC guidance for people who ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.601119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>This was sent to me by a friend from Naples, Italy. The “protezione civile” (civil protection service) has been sending cars (even late at night) with loud speakers urging residents to stay indoor...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.986594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Desperate Trump supporters are trying to label COVID-19 the Wuhan virus to distract from the fact that the Trump administration's failures have increased the danger here in the U.S. It's childish,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.356949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>The COVID-19 has been around forever. The COVID-19 is a different strain just like there’s different strains of the flu. STOP the panic. Take precaution and follow the guidelines for prevention! m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>On COVID2019, said: “One of the theories is perhaps you could take it on the chin, take it all in one go and allow coronvirus to move through the population without really taking as many draconian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.364582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Democrats and the Media need to stop using the COVID-19 to politicize things and scare people. It's irresponsible. This is not the time to try and gain political points or headlines from scaring p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>US researchers say that it takes an average of 5 days for COVID-19 symptoms (fever, cough, shortness of breath) to appear in COVID-19-infected people. Anyone symptom-free by day 12 is unlikely to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.988215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Imagine Metro Manila palang may confirmed case. Getting scared kasi di mawari kung paano ang testing process dito sa ibang region Yes guys Philippines is not ready for this kind of situation yet o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Italy's Prime Minister Giuseppe Conte has announced that the whole of the country is being put on lockdown in an attempt to contain the COVID-19 outbreak. For the latest on COVID-19, click here:</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.987125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Confirmed cases: 25/2/2020: Italy 320; UK 12 9/3/2020: Italy &amp;gt;7,000; UK 319 Italy don’t have more cases. They’re just two weeks ahead of us. And yet UK govt still doing nothing. COVID-19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Zee News : Petrol price reduced by Rs 2.69 CNN : Petrol price reduced by Rs 2.69 BBC : Petrol price reduced by Rs 2.69 NDTV : China is sending COVID-19 to world via mails and whatsapp</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>On a morning when Americans are terrified, the markets are gonna historically crash and we need LEADERSHIP...all you’ve done is hate-tweet BULLSHIT about Sanders, Warren, Biden, Democrats, Schumer...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>At CPAC, Mick Mulvaney rattled off his cuckoo COVID-19 hoax theory, claiming liberals and the media are weaponizing COVID-19 to take Trump down. Since then, several CPAC attendees have self-quaran...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.987779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Italian doctor facing COVID-19 tsunami publishes long, moving thread that culminates with, ‘Is panic really worse than neglect and carelessness during an epidemic of this sort?’ Read the whole fri...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>New information: All three individuals who have tested positive for COVID-19 in Ohio are from Cuyahoga County.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>the number of COVID-19 cases in the US surpasses 1,000 with 1,004 people in 37 states and DC testing positive for COVID-19, plus 31 deaths. This is just beginning the acceleration phase of the Cor...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.986627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>As two epidemics - COVID-19 and Brexit - hit us, don’t let them make you forget: Priti Patel scandal and investigations of Johnson’s Arcuri Russia report referendum crimes lies incompetence etc et...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>คำขวัญ Thailand 2020 No privacy No security No democracy No hope No future No mask But we had COVID-19 and stupid government Thank you ธนาธร ไม่เอารัฐประหาร</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>AMAZING ANIMATION on why it’s critical to FlattenTheCurve for fighting the COVID-19 epidemic. Caseload over healthcare capacity is what to excess higher mortality. Acting early- such as more testi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>“The more we can delay the peak of the spread to the summer, the better the NHS will be able to manage,” says Boris Johnson. If only your party hadn’t spent the last 10 years running the NHS into ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>“If you don’t wear a mask in public, the people will not talk to you.” Meet the Pakistani family in Hong Kong giving away thousands of face masks and sanitary products for free to those in need am...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>COVID-19 shows how secure borders also protects the public health. From my speech.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Employer posts tip sheet for COVID-19 safety and tapes over the part about staying home when sick. Our society couldn't be structured worse for preventing the spread of a virulent pandemic if we w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Life Care Center in Kirkland, Washington has received COVID-19 test results for 35 residents: 31 positive 1 negative 3 inconclusive They are still awaiting test results for the remaining 20 Life C...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Pandemics spread precisely where people have a false sense of security. Here's Milan on a nice day last week. FlattenTheCurve we need everyone's help; here's how:</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Italy had its highest single-day increase in deaths from 463 to 631 and now has &amp;gt;10,000 COVID-19 cases—the most confirmed in any country outside China. COVID-19 cases will surge for the rest of...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.988758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>The Democrats want a market crash. If they can use the COVID-19 to create more chaos, they will jump on it. They will hire social media trolls, up the media rhetoric to manufacture a crisis. Why? ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.446596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Too many conflicting reports coming from govt officials re: COVID-19/COVID-19. We need to know ground truth. Whistleblowers please come forward. We'll provide you pro bono (free) legal representat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>The total Iranian COVID-19 case-count is in the hundreds of thousands, perhaps millions, according to my estimates (detailed at the link). This raises an important question: if there are two milli...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                  tweet_text  \\\n",
       "15   Trust Merkel to be the first world leader to say openly what's what: the main point of social distancing measures (quarantines, closures, lockdowns) is to slow the spread of the COVID-19 so that h...   \n",
       "19   The empire is striking back. The COVID-19 is now being used as a weapon to destabilize the US economy because that the powers that be feel that’s the only way they can get rid of Trump and regain ...   \n",
       "20                                                                            private ny colleges: *closed college for the week because of the COVID-19* CUNY: *installed two new hand sanitizer dispensers*   \n",
       "22                             This thread needs to fly. It shows how the legacy media is USING COVID-19 as a political weapon and even how the SAME reporters are contradicting themselves. This. Is. SICK.   \n",
       "23   DEVELOPING: confirms directly to that he is self-quarantined after testing negative COVID-19. Meadows has not slowed down, with conference calls preparing for transition WH Chief of Staff. Meadows...   \n",
       "26   Older adults and people of all ages with severe chronic medical conditions are more likely to develop serious outcomes, including death, if infected with COVID-19. See CDC guidance for people who ...   \n",
       "29   This was sent to me by a friend from Naples, Italy. The “protezione civile” (civil protection service) has been sending cars (even late at night) with loud speakers urging residents to stay indoor...   \n",
       "31   Desperate Trump supporters are trying to label COVID-19 the Wuhan virus to distract from the fact that the Trump administration's failures have increased the danger here in the U.S. It's childish,...   \n",
       "32   The COVID-19 has been around forever. The COVID-19 is a different strain just like there’s different strains of the flu. STOP the panic. Take precaution and follow the guidelines for prevention! m...   \n",
       "36   On COVID2019, said: “One of the theories is perhaps you could take it on the chin, take it all in one go and allow coronvirus to move through the population without really taking as many draconian...   \n",
       "38   Democrats and the Media need to stop using the COVID-19 to politicize things and scare people. It's irresponsible. This is not the time to try and gain political points or headlines from scaring p...   \n",
       "42   US researchers say that it takes an average of 5 days for COVID-19 symptoms (fever, cough, shortness of breath) to appear in COVID-19-infected people. Anyone symptom-free by day 12 is unlikely to ...   \n",
       "44   Imagine Metro Manila palang may confirmed case. Getting scared kasi di mawari kung paano ang testing process dito sa ibang region Yes guys Philippines is not ready for this kind of situation yet o...   \n",
       "47        Italy's Prime Minister Giuseppe Conte has announced that the whole of the country is being put on lockdown in an attempt to contain the COVID-19 outbreak. For the latest on COVID-19, click here:   \n",
       "51             Confirmed cases: 25/2/2020: Italy 320; UK 12 9/3/2020: Italy &gt;7,000; UK 319 Italy don’t have more cases. They’re just two weeks ahead of us. And yet UK govt still doing nothing. COVID-19   \n",
       "55                   Zee News : Petrol price reduced by Rs 2.69 CNN : Petrol price reduced by Rs 2.69 BBC : Petrol price reduced by Rs 2.69 NDTV : China is sending COVID-19 to world via mails and whatsapp   \n",
       "56   On a morning when Americans are terrified, the markets are gonna historically crash and we need LEADERSHIP...all you’ve done is hate-tweet BULLSHIT about Sanders, Warren, Biden, Democrats, Schumer...   \n",
       "59   At CPAC, Mick Mulvaney rattled off his cuckoo COVID-19 hoax theory, claiming liberals and the media are weaponizing COVID-19 to take Trump down. Since then, several CPAC attendees have self-quaran...   \n",
       "63   Italian doctor facing COVID-19 tsunami publishes long, moving thread that culminates with, ‘Is panic really worse than neglect and carelessness during an epidemic of this sort?’ Read the whole fri...   \n",
       "75                                                                                            New information: All three individuals who have tested positive for COVID-19 in Ohio are from Cuyahoga County.   \n",
       "76   the number of COVID-19 cases in the US surpasses 1,000 with 1,004 people in 37 states and DC testing positive for COVID-19, plus 31 deaths. This is just beginning the acceleration phase of the Cor...   \n",
       "78   As two epidemics - COVID-19 and Brexit - hit us, don’t let them make you forget: Priti Patel scandal and investigations of Johnson’s Arcuri Russia report referendum crimes lies incompetence etc et...   \n",
       "88                                              คำขวัญ Thailand 2020 No privacy No security No democracy No hope No future No mask But we had COVID-19 and stupid government Thank you ธนาธร ไม่เอารัฐประหาร   \n",
       "95   AMAZING ANIMATION on why it’s critical to FlattenTheCurve for fighting the COVID-19 epidemic. Caseload over healthcare capacity is what to excess higher mortality. Acting early- such as more testi...   \n",
       "100  “The more we can delay the peak of the spread to the summer, the better the NHS will be able to manage,” says Boris Johnson. If only your party hadn’t spent the last 10 years running the NHS into ...   \n",
       "101  “If you don’t wear a mask in public, the people will not talk to you.” Meet the Pakistani family in Hong Kong giving away thousands of face masks and sanitary products for free to those in need am...   \n",
       "104                                                                                                                       COVID-19 shows how secure borders also protects the public health. From my speech.   \n",
       "107  Employer posts tip sheet for COVID-19 safety and tapes over the part about staying home when sick. Our society couldn't be structured worse for preventing the spread of a virulent pandemic if we w...   \n",
       "119  Life Care Center in Kirkland, Washington has received COVID-19 test results for 35 residents: 31 positive 1 negative 3 inconclusive They are still awaiting test results for the remaining 20 Life C...   \n",
       "120                                       Pandemics spread precisely where people have a false sense of security. Here's Milan on a nice day last week. FlattenTheCurve we need everyone's help; here's how:   \n",
       "121  Italy had its highest single-day increase in deaths from 463 to 631 and now has &gt;10,000 COVID-19 cases—the most confirmed in any country outside China. COVID-19 cases will surge for the rest of...   \n",
       "131  The Democrats want a market crash. If they can use the COVID-19 to create more chaos, they will jump on it. They will hire social media trolls, up the media rhetoric to manufacture a crisis. Why? ...   \n",
       "134  Too many conflicting reports coming from govt officials re: COVID-19/COVID-19. We need to know ground truth. Whistleblowers please come forward. We'll provide you pro bono (free) legal representat...   \n",
       "139  The total Iranian COVID-19 case-count is in the hundreds of thousands, perhaps millions, according to my estimates (detailed at the link). This raises an important question: if there are two milli...   \n",
       "\n",
       "     check_worthiness  predictions  probability  \n",
       "15                  1            0     0.009174  \n",
       "19                  0            1     0.986261  \n",
       "20                  0            1     0.987550  \n",
       "22                  1            0     0.006693  \n",
       "23                  0            1     0.990744  \n",
       "26                  0            1     0.601119  \n",
       "29                  0            1     0.986594  \n",
       "31                  1            0     0.356949  \n",
       "32                  1            0     0.007784  \n",
       "36                  1            0     0.364582  \n",
       "38                  1            0     0.006674  \n",
       "42                  0            1     0.988215  \n",
       "44                  1            0     0.008047  \n",
       "47                  0            1     0.987125  \n",
       "51                  0            1     0.990636  \n",
       "55                  0            1     0.991063  \n",
       "56                  1            0     0.006409  \n",
       "59                  0            1     0.987779  \n",
       "63                  1            0     0.006344  \n",
       "75                  0            1     0.824143  \n",
       "76                  0            1     0.986627  \n",
       "78                  0            1     0.857124  \n",
       "88                  1            0     0.006982  \n",
       "95                  1            0     0.006452  \n",
       "100                 1            0     0.009444  \n",
       "101                 0            1     0.979241  \n",
       "104                 1            0     0.006842  \n",
       "107                 1            0     0.006948  \n",
       "119                 0            1     0.990585  \n",
       "120                 1            0     0.006476  \n",
       "121                 0            1     0.988758  \n",
       "131                 1            0     0.446596  \n",
       "134                 1            0     0.007319  \n",
       "139                 1            0     0.008649  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df['predictions'] != test_df['check_worthiness']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\repos\\huggingface_test\\TransformersForClaimWorthiness\\TransformersForClaimWorthiness.ipynb Cell 58\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/repos/huggingface_test/TransformersForClaimWorthiness/TransformersForClaimWorthiness.ipynb#ch0000069?line=0'>1</a>\u001b[0m test_df[test_df[\u001b[39m'\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m test_df[\u001b[39m'\u001b[39m\u001b[39mcheck_worthiness\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "test_df[test_df['predictions'] != test_df['check_worthiness']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (transformer): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1945390927233831, inplace=False)\n",
       "    (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worthiness_checker.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "isCheckWorthy = (probability> .5)\n",
    "isCheckWorthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This expression contains a check-worthy claim with a 97.25% conficency \n"
     ]
    }
   ],
   "source": [
    "print('This expression contains a check-worthy claim with a {:.2%} conficency '.format(probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model(input_ids.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.transformer(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = get_data_from_file(optimized_config.data_version)\n",
    "train_dataset = create_dataset(train_df, tokenizer, optimized_config)\n",
    "test_dataset = create_dataset(test_df, tokenizer, optimized_config)\n",
    "train_dataloader, test_dataloader = ret_dataloader(optimized_config.batch_size, train_dataset, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = custom_models.TransformerClassifier(optimized_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6fd66f23f291c7a70d0834aa3c84fb3d8a0e23845cff3812a1a92aee36ee1d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
