{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    #sys.path.insert(0, \"..\")\n",
    "    sys.path.append('../')\n",
    "import logging\n",
    "logging.basicConfig(level='ERROR')\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import wandb\n",
    "import wget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import (AutoModel, AutoTokenizer, AutoModelForSequenceClassification, \n",
    "    get_linear_schedule_with_warmup, AutoConfig)\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, log_loss,\n",
    "matthews_corrcoef, average_precision_score)\n",
    "\n",
    "#Custom modules\n",
    "import utils\n",
    "from utils import custom_models, early_stopping, worthiness_checker, constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = 'TransformersForClaimWorthiness.ipynb'\n",
    "\n",
    "# Constants\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "parent_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "seed_list = [42] \n",
    "fold_count = 6\n",
    "patience=5\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "metric_types = np.dtype(\n",
    "    [\n",
    "        (\"mAP\", float),\n",
    "        (\"auc\", float),\n",
    "        (\"accuracy\", float),\n",
    "        (\"precision\", float),\n",
    "        (\"recall\", float),\n",
    "        (\"f1\", float),\n",
    "        (\"mcc\", float),\n",
    "        (\"log_loss\", float),\n",
    "        (\"loss\", float)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = constants.Constants()\n",
    "constants.device = device\n",
    "constants.parent_dir = parent_dir\n",
    "constants.seed_list = seed_list\n",
    "constants.fold_count = fold_count\n",
    "constants.patience = patience\n",
    "constants.loss_function = loss_function\n",
    "constants.metric_types = metric_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(parent_dir, 'Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(data_version):\n",
    "    train_df = pd.read_csv(os.path.join(parent_dir, 'Data','train_english_{}.tsv'.format(data_version)), delimiter='\\t')\n",
    "    test_df = pd.read_csv(os.path.join(parent_dir, 'Data','test_english_{}.tsv'.format(data_version)), delimiter='\\t')\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_list(df: pd.DataFrame, fold_count, random_state):\n",
    "    # Group positive and negative samples for stratified sampling\n",
    "    sampling_df = df.sample(frac=1, replace=False, random_state=random_state)\n",
    "    sampling_negative_df = sampling_df[sampling_df['check_worthiness']==0]\n",
    "    sampling_positive_df = sampling_df[sampling_df['check_worthiness']==1]\n",
    "\n",
    "    #determine fold length for both classes\n",
    "    fold_size_for_negatives = sampling_negative_df.shape[0]//fold_count\n",
    "    fold_size_for_positives = sampling_positive_df.shape[0]//fold_count\n",
    "\n",
    "    fold_list = []\n",
    "    for i in range(fold_count):\n",
    "        lower_bound_positives = fold_size_for_positives*i\n",
    "        upper_bound_positives = fold_size_for_positives * (i+1)\n",
    "        \n",
    "        lower_bound_negatives = fold_size_for_negatives*i\n",
    "        upper_bound_negatives = fold_size_for_negatives * (i+1)\n",
    "\n",
    "        if i+1 == fold_count:\n",
    "            upper_bound_positives = None\n",
    "            upper_bound_negatives = None\n",
    "\n",
    "        fold_for_negatives = sampling_negative_df.iloc[lower_bound_negatives : upper_bound_negatives]\n",
    "        fold_for_positives = sampling_positive_df.iloc[lower_bound_positives : upper_bound_positives]\n",
    "        fold_df = pd.concat([fold_for_negatives, fold_for_positives]).sample(frac=1, replace=False, random_state=random_state)\n",
    "\n",
    "        fold_list.append(fold_df)\n",
    "\n",
    "    return fold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_dataset(df, tokenizer, config):\n",
    "    max_token_length = config.max_token_length\n",
    "\n",
    "    sentences = df.tweet_text.values\n",
    "    labels = df.check_worthiness.values\n",
    "    incices = df.index.values.tolist()\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]' or equivalent\n",
    "                            max_length = max_token_length,           # 64? 4-128? Pad & truncate all sentences.\n",
    "                            truncation=True,\n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = False,   # Do not Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    labels = torch.tensor(labels).float()\n",
    "    incices = torch.tensor(incices).int()\n",
    "    dataset = TensorDataset(input_ids, labels, incices)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'name': 'Bert_hyperparameters',\n",
    "    'method': 'bayes', #grid, random\n",
    "    'program': 'TransformersForClaimWorthiness.ipynb',\n",
    "    'early_terminate': {\n",
    "      'type': 'hyperband',\n",
    "      'eta': 3,\n",
    "      's': 2,\n",
    "      'max_iter': 27   \n",
    "    },\n",
    "    'metric': {\n",
    "      'name': 'avg_val_mAP',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'data_version': {\n",
    "          'values': ['raw', 'cleaned_with_mentions', 'cleaned_without_mentions'],\n",
    "          'distribution': 'categorical'  \n",
    "        },\n",
    "        'max_token_length': {\n",
    "           'min': 4,\n",
    "           'max': 80,\n",
    "           'distribution': 'int_uniform'\n",
    "        },\n",
    "        'model_name': {\n",
    "          'values': ['bert-base-uncased'],\n",
    "          'distribution': 'categorical'  \n",
    "        },\n",
    "        'hidden_act': {\n",
    "          'values': ['relu', 'gelu', 'gelu_new', 'silu'],\n",
    "          'distribution': 'categorical'  \n",
    "        },\n",
    "        'position_embedding_type': {\n",
    "          'values': ['absolute', 'relative_key', 'relative_key_query'],\n",
    "          'distribution': 'categorical'  \n",
    "        },\n",
    "        'attention_dropout': {\n",
    "            'min': 0.001,\n",
    "            'max': 0.2\n",
    "        },\n",
    "        'transformer_dropout': {\n",
    "            'min': 0.001,\n",
    "            'max': 0.2\n",
    "        },\n",
    "        'classifier_dropout': {\n",
    "            'min': 0.001,\n",
    "            'max': 0.2\n",
    "        },\n",
    "        'layer_norm_eps': {\n",
    "            'min': 1e-14,\n",
    "            'max': 1e-10\n",
    "        },\n",
    "        'batch_size': {\n",
    "           'min': 2,\n",
    "           'max': 80,\n",
    "           'distribution': 'int_uniform'\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'min': 0.0000005,\n",
    "            'max': 0.00025\n",
    "        },\n",
    "        'epochs':{\n",
    "           'min': 1,\n",
    "           'max': 80,\n",
    "           'distribution': 'int_uniform'\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check_points = ['vinai/bertweet-covid19-base-uncased', 'roberta-base', 'bert-base-uncased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_defaults = {\n",
    "    'data_version': 'cleaned_with_mentions',\n",
    "    'max_token_length': 46,\n",
    "    'model_name': 'bert-base-uncased',\n",
    "    'hidden_act': 'gelu',\n",
    "    'position_embedding_type': 'absolute',\n",
    "    'layer_norm_eps': 5.4225686692811365e-11,\n",
    "    'learning_rate': 0.000028734737822604655,\n",
    "    'transformer_dropout': 0.03873251195245608,\n",
    "    'attention_dropout': 0.015328152075297112,\n",
    "    'classifier_dropout': 0.10850207289443518,\n",
    "    'batch_size': 53,\n",
    "    'epochs':25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANDB PARAMETER\n",
    "def ret_dataloader(batch_size, train_dataset, validation_dataset):\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,  # The training samples.\n",
    "                sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "                batch_size = batch_size # Trains with this batch size.\n",
    "            )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "                validation_dataset, # The validation samples.\n",
    "                sampler = SequentialSampler(validation_dataset), # Pull out batches sequentially.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            )\n",
    "    return train_dataloader, validation_dataloader\n",
    "\n",
    "def ret_optim(model, config):\n",
    "    #print('Learning_rate = ',wandb.config.learning_rate )\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                      lr = config.learning_rate, \n",
    "                      eps = 1e-8 \n",
    "                    )\n",
    "    return optimizer\n",
    "    \n",
    "def ret_scheduler(train_dataloader,optimizer, config):\n",
    "    epochs = config.epochs\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "    return scheduler\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(fold_list, fold_index, tokenizer, config):\n",
    "    temp_list = fold_list.copy()\n",
    "\n",
    "    trial_validation_df = temp_list.pop(fold_index)\n",
    "    trial_train_df = pd.concat(temp_list)\n",
    "\n",
    "    train_dataset = create_dataset(trial_train_df, tokenizer, config)\n",
    "    validation_dataset = create_dataset(trial_validation_df, tokenizer, config)\n",
    "\n",
    "    return ret_dataloader(config.batch_size, train_dataset, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_metrics(probability, label_list):\n",
    "    metrics_dictionary = {}\n",
    "    # predictions = np.argmax(probability.detach().cpu().numpy(), axis=0)\n",
    "    predictions =  [int(i > .5) for i in probability]\n",
    "\n",
    "    accuracy = accuracy_score(label_list, predictions)\n",
    "    precision = precision_score(label_list, predictions, zero_division=0)\n",
    "    recall = recall_score(label_list, predictions, zero_division=0)\n",
    "    f1 = f1_score(label_list, predictions, zero_division=0)\n",
    "    log_loss = metrics.log_loss(label_list, predictions)\n",
    "    mcc = matthews_corrcoef(label_list, predictions)\n",
    "    auc = roc_auc_score(label_list, probability)\n",
    "    \n",
    "    mAP = average_precision_score(label_list, probability)\n",
    "\n",
    "    metric_df = pd.DataFrame(np.empty(0, dtype=metric_types))\n",
    "    metric_df.loc[0] = [mAP, auc, accuracy, precision, recall, f1, mcc, log_loss, 0]\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(metric_df, prefix):\n",
    "    wandb.log({prefix + 'mAP':metric_df.loc[0, 'mAP'],\n",
    "            prefix + 'auc':metric_df.loc[0, 'auc'],\n",
    "            prefix + 'accuracy':metric_df.loc[0, 'accuracy'],\n",
    "            prefix + 'precision':metric_df.loc[0, 'precision'],\n",
    "            prefix + 'recall':metric_df.loc[0, 'recall'],\n",
    "            prefix + 'f1':metric_df.loc[0, 'f1'],\n",
    "            prefix + 'mcc':metric_df.loc[0, 'mcc'],\n",
    "            prefix + 'log_loss':metric_df.loc[0, 'log_loss'],\n",
    "            prefix + 'eval_loss':metric_df.loc[0, 'loss']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(model, device, dataloader):\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        probability_list = torch.Tensor(0)\n",
    "        label_list = np.empty(0)\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in dataloader:\n",
    "            \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_labels = batch[1].to(device)\n",
    "\n",
    "            with torch.no_grad():        \n",
    "                probability = model(b_input_ids).flatten()\n",
    "                loss = loss_function(probability, b_labels)\n",
    "\n",
    "            # Accumulate the validation loss, probability and labels.\n",
    "            total_eval_loss += loss.item()\n",
    "            probability_list = torch.cat((probability_list, probability.detach().cpu()), axis=0)\n",
    "            label_list = np.concatenate((label_list, batch[1]), axis=0)\n",
    "\n",
    "            # Move probability and labels to CPU\n",
    "            probability = probability.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # del b_input_ids\n",
    "            # del b_labels\n",
    "\n",
    "        # Calculate and log metrics and loss.\n",
    "        metrics_df = get_metrics(probability_list, label_list)\n",
    "        metrics_df.loc[0,'loss'] = total_eval_loss / len(dataloader)\n",
    "\n",
    "        return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, device, dataloader, loss_function, optimizer, scheduler):\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        probability = model(b_input_ids)\n",
    "        loss = loss_function(probability.flatten(), b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # del b_input_ids\n",
    "        # del b_labels\n",
    "\n",
    "    return total_train_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validate():\n",
    "    # clean gpu memory in any case if previous wandb run was crashed.\n",
    "    torch.cuda.empty_cache()\n",
    "    run = wandb.init(config=sweep_defaults)\n",
    "    run_start_time = time.time()\n",
    "    # print(wandb.config.items())\n",
    "    epochs = wandb.config.epochs\n",
    "\n",
    "    model_name = wandb.config.model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "    train_df, test_df = get_data_from_file(wandb.config.data_version)\n",
    "\n",
    "    run_train_metrics_list = []\n",
    "    run_val_metrics_list = []\n",
    "\n",
    "    for seed_index, seed in enumerate(seed_list):\n",
    "\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        fold_list = get_fold_list(train_df, 3, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "        for fold_index in range(len(fold_list)): \n",
    "\n",
    "            train_dataloader, validation_dataloader = create_dataloaders(fold_list, fold_index, tokenizer, wandb.config)\n",
    "            model = custom_models.TransformerClassifier(wandb.config).to(device)\n",
    "\n",
    "            optimizer = ret_optim(model, wandb.config)\n",
    "            scheduler = ret_scheduler(train_dataloader, optimizer, wandb.config)\n",
    "\n",
    "            # Creating class that checks early stopping condition\n",
    "            early_stopping = utils.early_stopping.EarlyStopping(patience=patience)\n",
    "\n",
    "            epoch_train_metrics_list = []\n",
    "            epoch_val_metrics_list = []\n",
    "\n",
    "            for epoch_i in range(0, epochs):\n",
    "\n",
    "                # ========================================\n",
    "                #               Training\n",
    "                # ========================================\n",
    "\n",
    "                # print(\"\")\n",
    "                print('======== Seed {:} - Fold {:} - Epoch {:} / {:} ========'.format(seed_index+1, fold_index+1, epoch_i + 1, epochs))\n",
    "                # print('Training...')\n",
    "                training_start_time = time.time()\n",
    "\n",
    "                epoch_train_loss = train_one_epoch(model, device, train_dataloader, loss_function, optimizer, scheduler)\n",
    "\n",
    "                training_time = format_time(time.time() - training_start_time)\n",
    "                wandb.log({'train_loss_':epoch_train_loss})\n",
    "\n",
    "                # print(\"  Average training loss: {0:.2f}\".format(epoch_train_loss))\n",
    "                #print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "                # ========================================\n",
    "                #               Evaluation\n",
    "                # ========================================\n",
    "\n",
    "                # print(\"Running Evaluation...\")\n",
    "                evaluation_start_time = time.time()\n",
    "\n",
    "                epoch_train_metrics = evaluate_one_epoch(model, device, train_dataloader)\n",
    "                log_metrics(epoch_train_metrics, 'train_')\n",
    "                epoch_train_metrics_list.append(epoch_train_metrics)\n",
    "\n",
    "                epoch_val_metrics = evaluate_one_epoch(model, device, validation_dataloader)\n",
    "                log_metrics(epoch_val_metrics, 'val_')\n",
    "                epoch_val_metrics_list.append(epoch_val_metrics)\n",
    "\n",
    "                val_mAP = epoch_val_metrics['mAP'].loc[0]\n",
    "                train_mAP = epoch_train_metrics['mAP'].loc[0]\n",
    "\n",
    "                print(\"  Training mAP: {:.3f} - Validation mAP: {:.3f}\".format(train_mAP,val_mAP ))\n",
    "\n",
    "                evaluation_time = format_time(time.time() - evaluation_start_time)\n",
    "                #print(\"  Evaluation took: {:}\".format(evaluation_time))\n",
    "                if early_stopping.should_stop(val_mAP):\n",
    "                    # print('terminating because of early stopping!')\n",
    "                    wandb.log({'early_stopped_at': seed_index*len(fold_list)*epochs+fold_index*epochs+epoch_i + 1})\n",
    "                    break\n",
    "\n",
    "            # at the end of each fold, after every epoch finished, \n",
    "            # get last epoch`s metrics as final metrics of current fold\n",
    "            #  # for  df in epoch_val_metrics_list:\n",
    "            #  #     log_metrics(epoch_val_metrics, 'val_{:}_{:}_'.format(seed_index+1, fold_index+1))\n",
    "\n",
    "            fold_train_metrics = epoch_train_metrics_list[-1]\n",
    "            fold_val_metrics = epoch_val_metrics_list[-1]\n",
    "\n",
    "            run_train_metrics_list.append(fold_train_metrics)\n",
    "            run_val_metrics_list.append(fold_val_metrics)\n",
    "\n",
    "            # at the end of every fold, \n",
    "            # calculate average metrics as final metrics of current run\n",
    "            run_train_metrics = pd.concat(run_train_metrics_list)\n",
    "            run_val_metrics = pd.concat(run_val_metrics_list)\n",
    "\n",
    "            log_metrics(pd.DataFrame([run_train_metrics.mean()]), 'avg_train_')\n",
    "            log_metrics(pd.DataFrame([run_val_metrics.mean()]), 'avg_val_')\n",
    "\n",
    "            # del model\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "    # print(\"\")\n",
    "    # print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-run_start_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "project=\"Transformers_For_ClaimWorthiness\"\n",
    "entity=\"cemulu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_id = wandb.sweep(sweep_config, project=project)\n",
    "sweep_id = 'nbovtee3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, project=project,function=cross_validate, count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model with best config and whole training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "# best_sweep = 'nbovtee3' #bert\n",
    "# best_sweep = 'embywnlj' #roberta\n",
    "best_sweep = '2afv0m0i' #bertweet\n",
    "sweep = api.sweep(\"cemulu/Transformers_For_ClaimWorthiness/\" + best_sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by -summary_metrics.avg_val_mAP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7651173954688729"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run = sweep.best_run()\n",
    "best_run.summary.get(\"avg_val_mAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch configuration of the best run:\n",
      "36\n",
      "Early stopped at:\n",
      "                   36    66    123    156    192    240\n",
      "fold_index         1.0   2.0   3.0    4.0    5.0    6.0\n",
      "cumulative_epoch  12.0  45.0  90.0  118.0  155.0  195.0\n",
      "epoch_of_fold     12.0   9.0  18.0   10.0   11.0   15.0\n",
      "\n",
      "Average epoch used as a reference for early stopping:  8\n"
     ]
    }
   ],
   "source": [
    "worthiness_checker = utils.worthiness_checker.WorthinessChecker(best_run, constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized_model = worthiness_checker.train_full_model()\n",
    "\n",
    "# model_file_name = 'bert-base-uncased_0.7332254639950794.pt' # bert\n",
    "# model_file_name = 'roberta-base_0.7551132534277406.pt' # roberta\n",
    "model_file_name = 'vinai_bertweet-covid19-base-uncased_0.7651173954688729.pt' # bertweet\n",
    "PATH = os.path.join(parent_dir, 'Model', model_file_name)\n",
    "worthiness_checker.load_model(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = worthiness_checker.config.model_name.replace('/','_')\n",
    "mAP = best_run.summary.get(\"avg_val_mAP\")\n",
    "\n",
    "PATH = os.path.join(parent_dir, 'Model','{}_{}.pt'.format(model_name, mAP))\n",
    "# torch.save(optimized_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"UK Health Minister Nadine Dorries has tested positive for COVID-19.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"i am positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"sheep is black\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"Recent research suggests that 15 percent of abortions are the result of coercion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = '''A Democratic bill negotiated between Sens. Joe Manchin and Chuck Schumer would \"increase taxes on millions of Americans across every income bracket.\"'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = '''Nancy Pelosi and Democrats \"want to turn 150 million Americans into felons overnight\" with HR 1808.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = '''John Fetterman wants to “eliminate life sentences for murderers.\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = '''\"The Sun is out of place, the Moon is out of place and the stars are out of place. The compasses are off\" because of a shift in the Earth’s poles.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"China threatens to shoot Nancy Pelosi’s plane down if she visits Taiwan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"In Virginia, we actually do protect same-sex marriage.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability = worthiness_checker.predict(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, test_df = worthiness_checker.get_data_from_file(worthiness_checker.config.data_version)\n",
    "_, test_df = worthiness_checker.get_data_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = worthiness_checker.create_dataset(test_df)\n",
    "_, test_dataloader = ret_dataloader(worthiness_checker.config.batch_size, _, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_batch(batch, self):\n",
    "    b_input_ids = batch[0].to(self.constants.device)\n",
    "    b_labels = batch[1].to(self.constants.device)\n",
    "    with torch.no_grad():        \n",
    "        probability = self.model(b_input_ids).flatten()\n",
    "        loss = self.constants.loss_function(probability, b_labels)\n",
    "    probability = probability.detach().cpu()\n",
    "    return probability, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_list = torch.Tensor(0)\n",
    "for batch in test_dataloader:\n",
    "    probability, _ = evaluate_one_batch( batch, worthiness_checker)\n",
    "    probability_list = torch.cat((probability_list, probability), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probability_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  [int(i > .5) for i in probability_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_prefix = 'bert_'\n",
    "# model_prefix = 'roberta_'\n",
    "# model_prefix = 'bertweet_'\n",
    "model_prefix = 'bertweet_filtered_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_df = test_df.copy()\n",
    "eval_df = pd.read_csv(os.path.join(data_dir,\"eval_df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[model_prefix + 'predictions'] = predictions\n",
    "eval_df[model_prefix + 'probability'] = probability_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.insert(0, 'tweet_url', test_df[\"tweet_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.insert(0, 'tweet_id', test_df[\"tweet_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(\"eval_df.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Positives:\n",
    "\n",
    "* (output = 0.986627) the number of COVID-19 cases in the US surpasses 1,000 with 1,004 people in 37 states and DC testing positive for COVID-19, plus 31 deaths. This is just beginning the acceleration phase of the Cor...\n",
    "\n",
    "* (output = 0.987125) Italy's Prime Minister Giuseppe Conte has announced that the whole of the country is being put on lockdown in an attempt to contain the COVID-19 outbreak. For the latest on COVID-19, click here:\n",
    "\n",
    "***\n",
    "\n",
    "* (output = 0.986261) The empire is striking back. The COVID-19 is now being used as a weapon to destabilize the US economy because that the powers that be feel that’s the only way they can get rid of Trump and regain ...\n",
    "\n",
    "* (output = 0.857124) As two epidemics - COVID-19 and Brexit - hit us, don’t let them make you forget: Priti Patel scandal and investigations of Johnson’s Arcuri Russia report referendum crimes lies incompetence etc et..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Negatives\n",
    "\n",
    "* (output = 0.006674) Democrats and the Media need to stop using the COVID-19 to politicize things and scare people. It's irresponsible. This is not the time to try and gain political points or headlines from scaring p...\n",
    "\n",
    "* (output = 0.006982) คำขวัญ Thailand 2020 No privacy No security No democracy No hope No future No mask But we had COVID-19 and stupid government Thank you ธนาธร ไม่เอารัฐประหาร\n",
    "\n",
    "***\n",
    "\n",
    "* (output = 0.006344) Italian doctor facing COVID-19 tsunami publishes long, moving thread that culminates with, ‘Is panic really worse than neglect and carelessness during an epidemic of this sort?’ Read the whole fri...\n",
    "\n",
    "* (output = 0.006693) This thread needs to fly. It shows how the legacy media is USING COVID-19 as a political weapon and even how the SAME reporters are contradicting themselves. This. Is. SICK.\n",
    "\n",
    "Ommitted Link: THREAD: Fri Jan 31, 2020, a few weeks before #Coronavirus has officially spread to other countries (which led to the bad stock market week Feb 24-Feb 28), the Trump Admin announced travel restrictions on China. Here is some of the reporting it generated. Take Politico of 2/4/20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet: \"\"private ny colleges: *closed college for the week because of the COVID-19* CUNY: *installed two new hand sanitizer dispensers*\"\"\n",
    "Label: 0\n",
    "Prediction: 1 with 98.75% probability\n",
    "\n",
    "Zero-shot GPT-3 response: \"The sentiment of this tweet is that private colleges are not doing enough to prevent the spread of COVID-19, while CUNY is taking steps to protect its students. This is not a check-worthy claim.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df['predictions'] != test_df['check_worthiness']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validated predictions for labeling error detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validated_predictions(config, k_fold, seed):\n",
    "    run_start_time = time.time()\n",
    "    epochs = config.epochs\n",
    "\n",
    "    model_name = config.model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "    train_df, test_df = get_data_from_file(config.data_version)\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    fold_list = get_fold_list(train_df, k_fold, random_state=seed)\n",
    "\n",
    "    probability_list = torch.Tensor(0)\n",
    "    label_list = np.empty(0)\n",
    "    index_list = np.empty(0, dtype=np.int32)\n",
    "\n",
    "    for fold_index in range(len(fold_list)): \n",
    "\n",
    "        train_dataloader, validation_dataloader = create_dataloaders(fold_list, fold_index, tokenizer, config)\n",
    "        model = custom_models.TransformerClassifier(config).to(device)\n",
    "\n",
    "        optimizer = ret_optim(model, config)\n",
    "        scheduler = ret_scheduler(train_dataloader, optimizer, config)\n",
    "\n",
    "        for epoch_i in range(0, epochs):\n",
    "\n",
    "            # Training\n",
    "            print('======== Fold {:} - Epoch {:} / {:} ========'.format(fold_index+1, epoch_i + 1, epochs))\n",
    "            epoch_train_loss = train_one_epoch(model, device, train_dataloader, loss_function, optimizer, scheduler)\n",
    "            print(\"  Average training loss: {0:.2f}\".format(epoch_train_loss))\n",
    "\n",
    "        # Evaluation\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for fold\n",
    "        for batch in validation_dataloader:\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_labels = batch[1].to(device)\n",
    "\n",
    "            with torch.no_grad():        \n",
    "                probability = model(b_input_ids).flatten()\n",
    "                loss = loss_function(probability, b_labels)\n",
    "\n",
    "            # Accumulate the validation loss, probability and labels.\n",
    "            total_eval_loss += loss.item()\n",
    "            probability_list = torch.cat((probability_list, probability.detach().cpu()), axis=0)\n",
    "            label_list = np.concatenate((label_list, batch[1]), axis=0)\n",
    "            index_list = np.concatenate((index_list, batch[2]), axis=0)\n",
    "\n",
    "            # Move probability and labels to CPU\n",
    "            probability = probability.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        avg_eval_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "        print(\" Validation loss: {:.3f}\".format(avg_eval_loss))\n",
    "\n",
    "    return index_list, probability_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list, probability_list = cross_validated_predictions(worthiness_checker.config, 10, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cleaned_without_mentions'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worthiness_checker.config.data_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  [int(i > .5) for i in probability_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = pd.DataFrame({'bertweet_predictions': predictions, 'bertweet_probability':probability_list.numpy()}, index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_df = prob_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(parent_dir, 'Data','train_english_cleaned_without_mentions.tsv'), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train_df = pd.concat([train_df, ordered_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train_df.to_csv(\"train_predictions_df.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>check_worthiness</th>\n",
       "      <th>bertweet_predictions</th>\n",
       "      <th>bertweet_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since this will never get reported by the medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.635311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thanks, MichaelBloomberg. Here’s a handy littl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Folks, when you say \"The COVID-19 isn't a big ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just 1 case of COVID-19 in India and people ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.849137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>President made a commitment to donate his sala...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.256311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>Stop spreading fake news COVID-19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>It's fake! It's fake!' shout residents of a co...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.983913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>Be Smart about COVID-19: 1⃣ follow accurate pu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>On the left: , a Qatari puppet, attacks Saudi ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.988063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>Could this be reason why COVID-19 has spread i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>822 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tweet_text  check_worthiness  \\\n",
       "0    Since this will never get reported by the medi...                 1   \n",
       "1    Thanks, MichaelBloomberg. Here’s a handy littl...                 0   \n",
       "2    Folks, when you say \"The COVID-19 isn't a big ...                 0   \n",
       "3    Just 1 case of COVID-19 in India and people ar...                 0   \n",
       "4    President made a commitment to donate his sala...                 1   \n",
       "..                                                 ...               ...   \n",
       "817                  Stop spreading fake news COVID-19                 0   \n",
       "818  It's fake! It's fake!' shout residents of a co...                 1   \n",
       "819  Be Smart about COVID-19: 1⃣ follow accurate pu...                 0   \n",
       "820  On the left: , a Qatari puppet, attacks Saudi ...                 1   \n",
       "821  Could this be reason why COVID-19 has spread i...                 1   \n",
       "\n",
       "     bertweet_predictions  bertweet_probability  \n",
       "0                       1              0.635311  \n",
       "1                       0              0.010500  \n",
       "2                       0              0.024375  \n",
       "3                       1              0.849137  \n",
       "4                       0              0.256311  \n",
       "..                    ...                   ...  \n",
       "817                     0              0.102101  \n",
       "818                     1              0.983913  \n",
       "819                     0              0.006635  \n",
       "820                     1              0.988063  \n",
       "821                     1              0.937985  \n",
       "\n",
       "[822 rows x 4 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch configuration of the best run:\n",
      "36\n",
      "Early stopped at:\n",
      "                   36    66    123    156    192    240\n",
      "fold_index         1.0   2.0   3.0    4.0    5.0    6.0\n",
      "cumulative_epoch  12.0  45.0  90.0  118.0  155.0  195.0\n",
      "epoch_of_fold     12.0   9.0  18.0   10.0   11.0   15.0\n",
      "\n",
      "Average epoch used as a reference for early stopping:  8\n"
     ]
    }
   ],
   "source": [
    "worthiness_checker = utils.worthiness_checker.WorthinessChecker(best_run, constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting configuration to use filtered data\n",
    "worthiness_checker.config.data_version = \"filtered\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 8 ========\n",
      "  Training mAP: 0.843 - Test mAP: 0.570\n",
      "======== Epoch 2 / 8 ========\n",
      "  Training mAP: 0.946 - Test mAP: 0.703\n",
      "======== Epoch 3 / 8 ========\n",
      "  Training mAP: 0.952 - Test mAP: 0.675\n",
      "======== Epoch 4 / 8 ========\n",
      "  Training mAP: 0.993 - Test mAP: 0.704\n",
      "======== Epoch 5 / 8 ========\n",
      "  Training mAP: 1.000 - Test mAP: 0.726\n",
      "======== Epoch 6 / 8 ========\n",
      "  Training mAP: 1.000 - Test mAP: 0.725\n",
      "======== Epoch 7 / 8 ========\n",
      "  Training mAP: 1.000 - Test mAP: 0.726\n",
      "======== Epoch 8 / 8 ========\n",
      "  Training mAP: 1.000 - Test mAP: 0.726\n",
      "*** Training Metrics ***\n",
      "        mAP       auc  accuracy  precision    recall        f1       mcc  \\\n",
      "0  0.842641  0.880390  0.852239   0.861635  0.640187  0.734584  0.648722   \n",
      "0  0.946384  0.966818  0.916418   0.865741  0.873832  0.869767  0.808250   \n",
      "0  0.951621  0.971358  0.904478   0.962963  0.728972  0.829787  0.779471   \n",
      "0  0.993373  0.996874  0.970149   0.957547  0.948598  0.953052  0.931193   \n",
      "0  0.999578  0.999805  0.994030   0.990654  0.990654  0.990654  0.986268   \n",
      "0  0.999724  0.999867  0.992537   0.986047  0.990654  0.988345  0.982862   \n",
      "0  1.000000  1.000000  1.000000   1.000000  1.000000  1.000000  1.000000   \n",
      "0  1.000000  1.000000  1.000000   1.000000  1.000000  1.000000  1.000000   \n",
      "\n",
      "       log_loss      loss  \n",
      "0  5.103517e+00  0.513848  \n",
      "0  2.886858e+00  0.281475  \n",
      "0  3.299234e+00  0.281408  \n",
      "0  1.031019e+00  0.099682  \n",
      "0  2.062040e-01  0.040530  \n",
      "0  2.577556e-01  0.029989  \n",
      "0  9.992007e-16  0.011516  \n",
      "0  9.992007e-16  0.010012  \n",
      "*** Test Metrics ***\n",
      "        mAP       auc  accuracy  precision    recall        f1       mcc  \\\n",
      "0  0.570179  0.664583  0.650000   0.617021  0.483333  0.542056  0.270714   \n",
      "0  0.703435  0.746042  0.657143   0.588235  0.666667  0.625000  0.313547   \n",
      "0  0.675048  0.688958  0.664286   0.638298  0.500000  0.560748  0.301278   \n",
      "0  0.703791  0.753542  0.728571   0.689655  0.666667  0.677966  0.443705   \n",
      "0  0.725735  0.775625  0.692857   0.626866  0.700000  0.661417  0.383878   \n",
      "0  0.725293  0.770000  0.692857   0.626866  0.700000  0.661417  0.383878   \n",
      "0  0.726402  0.768646  0.707143   0.650794  0.683333  0.666667  0.406181   \n",
      "0  0.726115  0.764167  0.721429   0.672131  0.683333  0.677686  0.432478   \n",
      "\n",
      "    log_loss      loss  \n",
      "0  12.088675  0.638444  \n",
      "0  11.842026  0.624431  \n",
      "0  11.595258  0.777085  \n",
      "0   9.374914  0.709996  \n",
      "0  10.608481  0.981709  \n",
      "0  10.608481  1.177837  \n",
      "0  10.115053  1.329713  \n",
      "0   9.621631  1.387812  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (transformer): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0701542558710376, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=3.5894066339034747e-11, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.03312463661668133, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1945390927233831, inplace=False)\n",
       "    (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worthiness_checker.train_full_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Contextual Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list = []\n",
    "for index, row in eval_df.iterrows():\n",
    "    tweet_embedding = worthiness_checker.get_embedding(tweet)\n",
    "    tweet_embedding = tweet_embedding.detach().cpu().numpy().tolist()\n",
    "    embeddings_list.append(tweet_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"embeddings\"] = embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(\"eval_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6fd66f23f291c7a70d0834aa3c84fb3d8a0e23845cff3812a1a92aee36ee1d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
